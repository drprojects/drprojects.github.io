---
my_name: Damien Robert
title: Damien Robert
image: "images/icons/damien.png"
icon: "images/icons/damien.png"

to_ign: <a href="https://www.ign.fr">IGN</a>
to_ensg: <a href="https://www.ensg.eu/">ENSG</a>
to_lastig: <a href="https://www.umr-lastig.fr">LASTIG</a>
to_strudel: <a href="https://www.umr-lastig.fr/strudel">STRUDEL</a>
to_acte: <a href="https://www.umr-lastig.fr/acte">ACTE</a>
to_uge:  <a href="https://www.univ-gustave-eiffel.fr">Univ. Gustave Eiffel</a>
to_crigen: <a href="https://www.engie.com/en/innovation-transition-energetique/centres-de-recherche/crigen">ENGIE Lab
    CRIGEN</a>
to_ecovision: <a href="https://www.spacehub.uzh.ch/en/research-areas/earth-observation/EcoVision-Lab.html">EcoVision</a>
to_uzh: <a href="https://www.uzh.ch/en.html">University of Zurich</a>
to_loic: <a href="https://loiclandrieu.com/">Lo√Øc Landrieu</a>
to_bruno: <a href="https://www.umr-lastig.fr/bruno-vallet/">Bruno Vallet</a>
to_hugo: <a href="https://1a7r0ch3.github.io/">Hugo Raguet</a>
to_jan: <a href="https://www.ics.uzh.ch/en/research/research-groups/Jan-Dirk-Wegner.html">Jan Wegner</a>

to_sebastien: <a href="https://people.irisa.fr/Sebastien.Lefevre/">S√©bastien Lef√®vre</a>
to_cedric: <a href="https://sites.google.com/view/cedricdemonceaux/home">C√©dric Demonceaux</a>
to_patrick: <a href="https://ptrckprz.github.io/">Patrick P√©rez</a>
to_siyu: <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>
to_duygu: <a href="https://www.duygu-ceylan.com/">Duygu Ceylan</a>

to_deepviewagg_paper: <a href="https://arxiv.org/abs/2204.07548">Paper</a>
to_deepviewagg_webpage: <a href="https://drprojects.github.io/deepviewagg">Webpage</a>
to_deepviewagg_code: <a href="https://github.com/drprojects/DeepViewAgg">Code</a>
to_deepviewagg_video: <a href="https://www.youtube.com/watch?v=SoMKwI863tw">Video</a>
to_deepviewagg_sota_s3dis: <a
        href="https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?p=learning-multi-view-aggregation-in-the-wild">S3DIS</a>
to_deepviewagg_sota_kitti360: <a
        href="https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?p=learning-multi-view-aggregation-in-the-wild">KITTI-360</a>

to_spt_paper: <a href="https://arxiv.org/abs/2306.08045">Paper</a>
to_spt_webpage: <a href="https://drprojects.github.io/superpoint-transformer">Webpage</a>
to_spt_code: <a href="https://github.com/drprojects/superpoint_transformer">Code</a>
to_spt_video: <a href="">Video</a>
to_spt_sota_s3dis: <a
        href="https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?p=efficient-3d-semantic-segmentation-with">S3DIS 6-Fold</a>
to_spt_sota_kitti360: <a
        href="https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?p=efficient-3d-semantic-segmentation-with">KITTI-360 Val</a>
to_spt_sota_dales: <a
        href="https://paperswithcode.com/sota/3d-semantic-segmentation-on-dales?p=efficient-3d-semantic-segmentation-with">DALES</a>
---

<!-- Imports -->
{% include_relative imports.html %}

<!-- Change colors here -->
<!--<div style="&#45;&#45;color_left:#80cc28;&#45;&#45;color_right:#00aec7">-->
<div style="--color_left:#f2ce02;--color_right:#d46606">

    <!-- Navigation menu -->
    {% include_relative navigation.html %}

    <!-- Header -->
    <section class="page-header">
        <h1 class="project-name">
            <img height="150" class=profilepicture src={{ page.image }}>{{ page.title }}
        </h1>
        <h2 class="project-tagline">
            {{ page.to_ecovision }} lab, {{ page.to_uzh }}, Zurich, Switzerland
        </h2>
        <a href=https://github.com/drprojects class="btn">
            <img height="50" src="images/icons/github.svg">GitHub
        </a>
        <a href=mailto:damien.robert@uzh.ch class="btn">
            <img height="50" src="images/icons/email.svg">Mail
        </a>
        <a href=https://www.linkedin.com/in/damienrobert91 class="btn">
            <img height="50" src="images/icons/linkedin.svg">LinkedIn
        </a>
        <a href=https://scholar.google.com/citations?user=Bmgo5wMAAAAJ&hl=en class="btn">
            <img height="50" src="images/icons/scholar.svg">Scholar
        </a>
        <a href=https://orcid.org/0000-0003-0983-7053 class="btn">
            <img height="50" src="images/icons/orcid.svg">ORCID
        </a>
        <a href=https://dblp.org/pid/48/7483-2.html class="btn">
            <img height="50" src="images/icons/dplb.png">DPLB
        </a>
        <a href=files/academic_resume_damien_robert.pdf class="btn">
            <img height="50" src="images/icons/resume.svg">Resume
        </a>
    </section>

    <!-- Page content -->
    <section class="main-content">
        <div class="cell border-box-sizing text_cell rendered">
            <div class="inner_cell">
                <div class="text_cell_render border-box-sizing rendered_html">

                    <!-- Summary -->
                    <br><br>
                    <h1 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h1>
                    <p class="blabla">
                        As a postdoctoral researcher at the {{ page.to_ecovision }} lab at {{ page.to_uzh }}, I collaborate with {{ page.to_jan }} to design deep learning methods for remote sensing and environmental applications.
                        Before joining UZH, I completed my PhD on <i>"Efficient Learning on Large-Scale 3D Point Clouds"</i> at {{ page.to_ign }} and {{ page.to_crigen }}, under the supervision of {{ page.to_loic }} and {{ page.to_bruno }}.
                        <br><br>
                        You like trees üå≥ ? You like satellites üõ∞Ô∏è ? You like me üòä
                    </p>

<!--                    &lt;!&ndash; Supervision &ndash;&gt;-->
<!--                    <br><br>-->
<!--                    <h1 id="Supervisors">Supervisors<a class="anchor-link" href="#Supervisors">&#182;</a></h1>-->
<!--                    <ul>-->
<!--                        <li>{{ page.to_loic }} ({{ page.to_strudel }}, {{ page.to_lastig }}, {{ page.to_uge }}, {{ page.to_ign }}-{{ page.to_ensg }}, Saint-Mand√©, France)-->
<!--                        </li>-->
<!--                        <li>{{ page.to_bruno }} ({{ page.to_acte }}, {{page.to_lastig }}, {{ page.to_uge }}, {{ page.to_ign }}-{{ page.to_ensg }}, Saint-Mand√©, France)-->
<!--                        </li>-->
<!--                    </ul>-->

                    <!-- Publications -->
                    <br><br>
                    <h1 id="Publications">üìÉ Publications<a class="anchor-link" href="#Publications">&#182;</a>
                    </h1>
                    üñº Poster | üé§ Oral
                    <br><br>
                    <div class="tom_table">

                        <div class="tom_row">
                            <div class="tom_cell" style="width:40%">
                                <a href="https://drprojects.github.io"><img src="images/supercluster/teaser.png" alt="SuperCluster" style="height:40vh"></a>
                            </div>
                            <div class="tom_cell_text">
                                <b><a href="https://drprojects.github.io/supercluster">Scalable 3D Panoptic Segmentation as Superpoint Graph Clustering</a></b><br>
                                <a href="">{{ page.my_name }}</a>, {{ page.to_hugo }}, {{ page.to_loic }}
<!--                                <br>-->
<!--                                {{ page.to_spt_paper }} | {{ page.to_spt_webpage }} | {{ page.to_spt_code }}-->
                                <br>
                                <b>3DV 2024 Oral üé§</b> (top 5.3% submissions)
                                <p class="blabla">
                                    SuperCluster proposes a framework for efficient panoptic segmentation of large-scale
                                    point clouds. We formulate the panoptic task as a graph clustering problem. We train
                                    a model to predict desirable node and edge attributes to be used as input for a
                                    downstream graph clustering algorithm. This allows for training a model with only
                                    local supervision, without the need for non-maximum suppression, instance matching,
                                    and without any prerequisite on the number of objects in the scene.

                                    SuperCluster achieves new SOTA panoptic segmentation on indoor datasets S3DIS Area 5
                                    (50.1 PQ (+7.8)) and ScanNetV2 (58.7 PQ (+25.2)), as well as outdoor datasets
                                    KITTI-360 (48.3 PQ) and DALES (61.2 PQ).
                                    <br>
                                    ü¶ã <b>210k param.</b> | ‚ö° <b>Train S3DIS F5 in 4h</b> | üíæ <b>20M-point inference on 1 GPU</b>
                                </p>
                            </div>
                        </div>
                        <br>
                        <br>
                        <div class="tom_row">
                            <div class="tom_cell" style="width:40%">
                                <a href="https://drprojects.github.io/superpoint-transformer"><img
                                        src="images/superpoint_transformer/teaser.png" alt="superpoint transformer" style="height:30vh"></a>
                            </div>
                            <div class="tom_cell_text">
                                <b><a href="https://drprojects.github.io/superpoint-transformer">Efficient 3D Semantic Segmentation with Superpoint Transformer</a></b><br>
                                <a href="">{{ page.my_name }}</a>, {{ page.to_hugo }}, {{ page.to_loic }}
                                <br>
                                {{ page.to_spt_paper }} | {{ page.to_spt_webpage }} | {{ page.to_spt_code }}
                                <br>
                                <b>ICCV 2023 Poster üñº</b> (top 26.8% submissions)
                                <p class="blabla">
                                    SPT is a superpoint-based transformer ü§ñ architecture that efficiently ‚ö°
                                    performs semantic segmentation on large-scale 3D scenes. This method includes a fast
                                    algorithm that partitions üß© point clouds into a hierarchical superpoint structure,
                                    as well as a self-attention mechanism to exploit the relationships between
                                    superpoints at multiple scales.
                                    We reach SOTA on {{ page.to_spt_sota_s3dis }} (76.0 mIoU),
                                    {{ page.to_spt_sota_kitti360 }} (63.5 mIoU), and {{ page.to_spt_sota_dales }}
                                    (79.6 mIoU)n with:
                                    <br>
                                    ü¶ã <b>212k param.</b> | ‚ö° <b>Train S3DIS F5 in 3h</b> | ‚åö <b><a href="https://github.com/loicland/superpoint_graph">SPG</a> preprocessing √∑7</b>
                                </p>
                            </div>
                        </div>
                        <br>
                        <br>
                        <div class="tom_row">
                            <div class="tom_cell" style="width:40%">
                                <a href="https://drprojects.github.io/deepviewagg"><img
                                        src="images/deepviewagg/teaser.png" alt="deepviewagg" style="height:30vh"></a>
                            </div>
                            <div class="tom_cell_text">
                                <b><a href="https://drprojects.github.io/deepviewagg">Learning Multi-View Aggregation In
                                    the Wild for Large-Scale 3D Semantic Segmentation</a></b><br>
                                <a href="">{{ page.my_name }}</a>, {{ page.to_bruno }}, {{ page.to_loic }}
                                <br>
                                {{ page.to_deepviewagg_paper }} | {{ page.to_deepviewagg_webpage }} | {{ page.to_deepviewagg_code }} | {{ page.to_deepviewagg_video }}
                                <br>
                                <b>CVPR 2022 Oral üé§ and Best Paper finalist üéâ</b> (top 0.4% submissions)
                                <p class="blabla">
                                    An end-to-end multi-view aggregation method for 3D semantic segmentation from images
                                    and point clouds. We reach SOTA on {{ page.to_deepviewagg_sota_s3dis }} and
                                    {{ page.to_deepviewagg_sota_kitti360 }} without requiring point cloud colorization,
                                    meshing, or depth sensors: just point clouds ‚òÅ, images üì∏, and their poses.
                                </p>
                            </div>
                        </div>

                    </div>

                    <!-- Resume -->
                    <br><br>
                    <h1 id="Short-CV">üìë Short Resume<a class="anchor-link" href="#Short-CV">&#182;</a></h1>
                    <ul style="list-style-type:none">
                        <li> <b>2024 - Now </b> Postdoctoral researcher on deep learning for remote sensing and environment with {{ page.to_jan }}</li>
                        <li> <b>2020 - 2024</b> PhD student on <i>"Efficient Learning on Large-Scale 3D Point Clouds"</i> supervised by {{ page.to_loic }} and {{ page.to_bruno }}</li>
                        <li> <b>2022 &emsp; &emsp; &ensp; </b> <a href="https://iplab.dmi.unict.it/icvss2022/">International Computer Vision Summer School</a></li>
                        <li> <b>2017 &emsp; &emsp; &ensp; </b> CNRS AI Fall School <a href="https://ia2.gdria.fr/">IA¬≤</a> , Multi-disciplinary course for AI students and researchers</li>
                        <li> <b>2011 - 2015</b> Master of Science at <a href="https://www.ec-lyon.fr/">Ecole Centrale de Lyon</a>
                        </li>
                    </ul>

                    <!-- Presentations, talks, tutorials -->
                    <br><br>
                    <h1 id="Talks">üñºüé§ Talks & Presentations<a class="anchor-link" href="#Talks">&#182;</a></h1>
                    üñº Poster | üé§ Oral
                    <br><br>
                    <ul style="list-style-type:none">
                        <li>üé§ <b>03/2024</b> Presenting our work <a href="https://drprojects.github.io/supercluster">SuperCluster</a> at <a href="https://3dvconf.github.io/2024">3DV 2024</a> (<b>Oral</b> üéâ)</li>
                        <li>üéâ <b>02/2024</b> Starting a new position as a postdoctoral researcher in the {{ page.to_ecovision }} lab at {{ page.to_uzh }} üöÄ</li>
                        <li>üé§ <b>01/2024</b> PhD defense on <i>Efficient Learning on Large-Scale 3D Point Clouds</i> at {{ page.to_ign }}, in presence of {{ page.to_cedric }}, {{ page.to_patrick }}, {{ page.to_siyu }}, {{ page.to_duygu }}, {{ page.to_loic }}, and {{ page.to_bruno }} üéì</li>
                        <li>üéâ <b>10/2023</b> Our work <a href="https://drprojects.github.io/supercluster">SuperCluster</a> was accepted for an oral presentation at <a href="https://3dvconf.github.io/2024">3DV 2024</a> üéâ</li>
                        <li>üé§ <b>10/2023</b> Presenting {{ page.to_ign }}'s research on <i>Large-Scale 2D and 3D Learning</i> to the <a href="https://www.maanmittauslaitos.fi/en">National Land Survey of Finland (NSL)</a></li>
                        <li>üé§ <b>10/2023</b> Presenting our work <a href="https://drprojects.github.io/supercluster">SuperCluster</a> to the <a href="https://imagine-lab.enpc.fr/">Ecole des Ponts, IMAGINE</a> lab</li>
                        <li>üé§ <b>07/2023</b> Presenting our work <a href="https://drprojects.github.io/superpoint-transformer">Superpoint Transformer</a> at <a href="https://iccv2023.thecvf.com/">ICCV 2023</a></li>
                        <li>üé§ <b>09/2023</b> Presenting our work <a href="https://drprojects.github.io/superpoint-transformer">Superpoint Transformer</a> to the <a href="https://cvg.ethz.ch/">ETH Z√ºrich, Computer Vision and Geometry</a> lab</li>
                        <li>üé§ <b>09/2023</b> Presenting our work <a href="https://drprojects.github.io/superpoint-transformer">Superpoint Transformer</a> to the <a href="https://prs.igp.ethz.ch/">ETH Z√ºrich, Photogrammetry and Remote Sensing</a> lab</li>
                        <li>üéâ <b>07/2023</b> Our work <a href="https://drprojects.github.io/superpoint-transformer">Superpoint Transformer</a> was accepted at <a href="https://iccv2023.thecvf.com/">ICCV 2023</a> üéâ</li>
                        <li>üé§ <b>06/2023</b> Presenting our work <a href="https://drprojects.github.io/superpoint-transformer">Superpoint Transformer</a> to the {{ page.to_crigen }}</li>
                        <li>üé§ <b>05/2023</b> Presenting our work <a href="https://drprojects.github.io/superpoint-transformer">Superpoint Transformer</a> to the <a href="https://www.spacehub.uzh.ch/en/research-areas/earth-observation/EcoVision-Lab.html">University of Z√ºrich, EcoVision</a> lab</li>
                        <li>üé§ <b>05/2023</b> Presenting our work <a href="https://drprojects.github.io/superpoint-transformer">Superpoint Transformer</a> to the <a href="https://samp.ai/">Samp</a> R&D lab</li>
                        <li>üé§ <b>05/2023</b> Presenting our work <a href="https://drprojects.github.io/superpoint-transformer">Superpoint Transformer</a> to the <a href="https://www.valeo.com/en/valeo-ai/">Valeo.ai</a> lab</li>
                        <li>üé§ <b>12/2022</b> Presenting <i>Self-Supervised Learning for Computer Vision</i> to the {{ page.to_lastig }} lab</li>
                        <li>üé§ <b>11/2022</b> Presenting {{ page.to_ign }}'s research on <i>Large-Scale 2D and 3D Learning</i> to the <a href="https://www.bkg.bund.de/EN/Home/home.html">German Federal Agency for Cartography and Geodesy (BKG)</a></li>
                        <li>üñº <b>07/2022</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> at <a href="https://iplab.dmi.unict.it/icvss2022/">ICVSS</a></li>
                        <li>üé§ <b>06/2022</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a> (<b>Best Paper finalist</b> üéâ)</li>
                        <li>üé§ <b>06/2022</b> Had the honor to be interviewed for the CV News <a href="https://www.rsipvision.com/ComputerVisionNews-2022July/24">Best of CVPR issue</a></li>
                        <li>üé§ <b>06/2022</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> to the <a href="https://imagine-lab.enpc.fr/">Ecole des Ponts, IMAGINE</a> lab</li>
                        <li>üñº <b>06/2022</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> at <a href="https://www.isprs2022-nice.com/">ISPRS 2022</a></li>
                        <li>üé§ <b>05/2022</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> to the <a href="https://www.lix.polytechnique.fr/">Ecole Polytechnique, LIX</a> lab</li>
                        <li>üé§ <b>04/2022</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> at the <a href="https://www.ai4geo.eu/en/">AI4GEO project</a> seminar</li>
                        <li>üñº <b>03/2022</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> at the <a href="https://www.ign.fr/agenda/31e-journee-de-la-recherche-de-lign-ensg">IGN-ENSG Research Days</a></li>
                        <li>üé§ <b>01/2022</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> at the <a href="https://www.gdr-isis.fr/">Information, Signal, Image and Vision research group</a> seminar</li>
                        <li>üñº <b>05/2021</b> Presenting our work <a href="https://drprojects.github.io/deepviewagg">DeepViewAgg</a> at the <a href="https://www.ign.fr/agenda/30es-journees-de-la-recherche-ign-ensg">IGN-ENSG Research Days</a></li>
                    </ul>

                    <!-- Teaching -->
                    <br><br>
                    <h1 id="Teaching">üìö Teaching<a class="anchor-link" href="#Teaching">&#182;</a></h1>
                    <ul style="list-style-type:none">
                        <li> <b>05/2024</b> <i>NeRFs, and Diffusion</i> at {{ page.to_uzh }} (Course Instructor, M2, 5.5 hours)</li>
                        <li> <b>01/2023</b> <i>Deep Learning for Remote Sensing</i> at {{ page.to_ensg }} (Course Instructor, M2, 13 hours)</li>
                        <li> <b>06/2022</b> <i>3D Deep Learning for Remote Sensing</i> at <a href="https://www.isprs2022-nice.com/">ISPRS 2022</a> (Tutorial Instructor, 1 day)</li>
                        <li> <b>05/2022</b> <i>3D Deep Learning, Torch-Points3D & DeepViewAgg</i> at {{ page.to_crigen }} (Tutorial Instructor, 1 day)</li>
                        <li> <b>01/2022</b> <i>Deep Learning for Remote Sensing</i> at {{ page.to_ensg }} (Course Instructor, M2, 9 hours)</li>
                        <li> <b>11/2020</b> <i>Deep Learning for Computer Vision</i> at <a href="https://www.polytechnique.edu/">Ecole Polytechnique</a> (Teaching Assistant, M1, 12 hours)</li>
                    </ul>

                    <!-- Affiliation -->
                    <br><br>
                    <h1 id="Affiliations">üè† Affiliations<a class="anchor-link" href="#Affiliations">&#182;</a></h1>
                    <ul>
                        <li>{{ page.to_ecovision }} lab, {{ page.to_uzh }}, Zurich, Switzerland</li>
                    </ul>
                    <br><br>
                    <ul class="hfig">
                        <li><a href="https://www.uzh.ch/en.html"><img style="height:8vh;" src="images/icons/uzh.png"></a></li>
                    </ul>

                </div>
            </div>
        </div>

        <!-- Footer -->
        {% include_relative footer.html %}
    </section>

</div>

