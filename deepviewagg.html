---
my_name: Damien Robert
title: Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation
image: "images/deepviewagg/cvpr_2022_no_background.png"
icon: "images/deepviewagg/teaser_mini.png"

to_ign: <a href="https://www.ign.fr">IGN</a>
to_ensg: <a href="https://www.ensg.eu/">ENSG</a>
to_lastig: <a href="https://www.umr-lastig.fr">LASTIG</a>
to_strudel: <a href="https://www.umr-lastig.fr/strudel">STRUDEL</a>
to_acte: <a href="https://www.umr-lastig.fr/acte">ACTE</a>
to_uge:  <a href="https://www.univ-gustave-eiffel.fr">Univ. Gustave Eiffel</a>
to_crigen: <a href="https://www.engie.com/en/innovation-transition-energetique/centres-de-recherche/crigen">ENGIE Lab
    CRIGEN</a>
to_loic: <a href="https://loiclandrieu.com/">Lo√Øc Landrieu</a>
to_bruno: <a href="https://www.umr-lastig.fr/bruno-vallet/">Bruno Vallet</a>

to_paper: <a href="https://arxiv.org/abs/2204.07548">Paper</a>
to_webpage: <a href="https://drprojects.github.io/deepviewagg">Webpage</a>
to_code: <a href="https://github.com/drprojects/DeepViewAgg">Code</a>
to_video: <a href="">Video</a>

to_sota_s3dis: <a
        href="https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?p=learning-multi-view-aggregation-in-the-wild">S3DIS</a>
to_sota_kitti360: <a
        href="https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?p=learning-multi-view-aggregation-in-the-wild">KITTI-360</a>
---
<!-- Imports -->
{% include_relative imports.html %}

<!-- Change colors here -->
<!--<div style="&#45;&#45;color_left:#80cc28;&#45;&#45;color_right:#00aec7">-->
<div style="--color_left:#f2ce02;--color_right:#d46606">

    <!-- Navigation menu -->
    {% include_relative navigation.html %}

    <!-- Header -->
    <section class="page-header">
        <h1 class="project-name">
            {{ page.title }}
        </h1>
        <h2 class="project-tagline">
            <img class=profilepicture src={{ page.image }} style="height:15vh; border-radius:0; border: 0;">
            <b>CVPR 2022 Oral üé§ and Best Paper finalist üéâ</b>
        </h2>
        <h2 class="project-tagline">
            <a href="https://drprojects.github.io/">{{ page.my_name }}</a><sup>1,2</sup>, {{ page.to_bruno }}<sup>2</sup>, {{ page.to_loic
            }}<sup>2</sup>
        </h2>
        <h2 class="project-tagline">
            <sup>1</sup>CSAI, {{ page.to_crigen }}, Stains, France
            <br>
            <sup>2</sup>{{ page.to_uge }}, {{ page.to_ign }}-{{ page.to_ensg }}, {{ page.to_lastig }}, F-94160
            Saint-Mande, France
        </h2>
        <a href=https://github.com/drprojects/DeepViewAgg class="btn">
            <img height="50" src="images/icons/github.svg">Code
        </a>
        <a href=https://arxiv.org/abs/2204.07548 class="btn">
            <img height="50" src="images/icons/paper.svg">Paper
        </a>
        <a href="" class="btn">
            <img height="50" src="images/icons/video.svg">Video
        </a>
    </section>

    <!-- Page content -->
    <section class="main-content">
        <div class="cell border-box-sizing text_cell rendered">
            <div class="inner_cell">
                <div class="text_cell_render border-box-sizing rendered_html">

                    <!-- Abstract -->
                    <h1 id="Abstract">Abstract<a class="anchor-link" href="#Abstract">&#182;</a>
                    </h1>
                    <div class="tom_table">
                        <div class="tom_row">
                            <div class="tom_cell" style="width:40%">
                                <img src="images/deepviewagg/teaser.png" alt="deepviewagg" style="height:40vh;">
                            </div>
                            <div class="tom_cell_text">
                                <p class="blabla">
                                    Recent works on 3D semantic segmentation propose to exploit the synergy between
                                    images üì∏ and point clouds ‚òÅ by processing each modality with a dedicated network
                                    and projecting learned 2D features onto 3D points. Merging large-scale point clouds
                                    and images raises several challenges, such as constructing a mapping between points
                                    and pixels, and aggregating features between multiple views. Current methods require
                                    mesh reconstruction or specialized sensors to recover occlusions, and use heuristics
                                    to select and aggregate available images. In contrast, we propose an end-to-end
                                    trainable multi-view aggregation model leveraging the viewing conditions of 3D
                                    points to merge features from images taken at arbitrary positions. Our method can
                                    combine standard 2D and 3D networks and outperforms both 3D models operating on
                                    colorized point clouds and hybrid 2D/3D networks without requiring colorization,
                                    meshing, or true depth maps. We set a new state-of-the-art for large-scale
                                    indoor/outdoor semantic segmentation on {{ page.to_sota_s3dis }} (74.7 mIoU 6-Fold)
                                    and on {{ page.to_sota_kitti360 }} (58.3 mIoU). Our full pipeline is accessible at
                                    on <a href="https://github.com/drprojects/DeepViewAgg">GitHub</a>, and only requires
                                    raw 3D scans and a set of images and poses.
                                </p>
                            </div>
                        </div>
                    </div>

                    <!-- Interactive visualization -->
                    <h1 id="Motivation">Motivation<a class="anchor-link" href="#Motivation">&#182;</a>
                    </h1>
                    <p class="blabla">
                        Modern 3D scene acquisition systems often produce images üì∏ along with point clouds ‚òÅ. Previous
                        works already have demonstrated that those modalities are complementary and extracting features
                        from both benefits the scene understanding.
                        <br><br>
                        As an example, it is not possible to distinguish a frame üñº from the wall üß± on which it is hung
                        solely based on the 3D geometric information in the point cloud ‚òÅ. However, the difference in
                        texture between these two objects can easily be identified in images üì∏ of the same scene.
                        <br><br>
                        On the contrary, when objects are poorly-lit, occluded or have uniform textures, it is much
                        easier to identify them by their 3D geometry ‚òÅ rather than their radiometry üì∏.
                        <br><br>
                        You can play with the interactive visualization below to get the intuition of how the 3D and 2D
                        modalities relate. This represents a 3D spherical sample of an indoor scene from the
                        <a href="http://buildingparser.stanford.edu/dataset.html">S3DIS</a> dataset. The colored balls
                        account for equirectangular picture positions. The corresponding images can be seen below the 3D
                        plot. <code>RGB</code> shows the points colorized from the image using human expertise and a
                        dedicated software. <code>Labels</code> shows the expected output for semantic segmentation.
                        <code>Times seen</code> shows how many images see each point. <code>Position RGB</code> colors
                        the points based on their 3D position. In the images below, the mapping between 3D points and
                        the corresponding pixels is shown using the <code>Times seen</code> color scheme. Note that
                        computing such a mapping is a non-trivial task as it requires recovering occlusions. If you want
                        to see the interactive visualization used in our poster, check out this
                        <a href="https://drprojects.github.io/deepviewagg-poster-sample">sample</a> üëà.
                        <br><br>
                        Although several works already exist on multimodal learning from point clouds and images, we
                        found that these tend to overlook the multi-view problem for large-scene analysis. Indeed,
                        when analysing large 3D scenes with images, each point may be seen in multiple images. You can
                        see an illustration of this in the <code>Times seen</code> mode in the visualization below. To
                        aggregate this multi-view information, one can use simple max-pool or average-pool schemes.
                        However, this disregards the fact that not all views of a same object are equal. Views can be
                        far, close, sideways, front-facing, occluded, distorted, etc. and carry different qualities of
                        information depending on those observation conditions.
                        <br><br>
                        Our core idea is simply to let a model learn to aggregate the multi-view information based on the
                        observation conditions üëÄ.
                    </p>

                    {% include_relative images/deepviewagg/s3dis_multimodal_sample__pos.html %}

                    <p class="blabla">
                        After playing with this tool, you may have noticed several things:
                    </p>
                    <ul>
                        <li>3D point clouds ‚òÅ and 2D images üì∏ carry complementary information. This justifies trying to
                            extract features from both modalities.</li>
                        <li>Views of a same object do not carry the same level of information, depending on their
                            observation conditions üëÄ. By the way, this implies that the colorization scheme used to
                            give RGB colors to the points is non-trivial, although often taken for granted by 3D
                            semantic segmentation methods.</li>
                        <li>Mapping 3D points with pixels of 2D images located in the wild requires recovering the
                            occlusions. This requires computing what is called a visibility model, which can be easily
                            solved if you have a mesh of the scene or if you have depth cameras. But mesh reconstruction
                            is not a simple task and not all acquisition systems are equipped with depth cameras.</li>
                        <li>Some images üì∏ have only a small portion of their pixels actually linked to the 3D spherical
                            sample at hand. This means extracting features maps from entire images with CNNs may require
                            a lot of unneeded computation.</li>
                    </ul>
                    <p class="blabla">
                        Our paper addresses these four points to learn multi-modal aggregation without mesh nor depth
                        cameras.
                    </p>

                    <!-- Resources -->
                    <h1 id="BibTex">BibTex<a class="anchor-link" href="#BibTex">&#182;</a>
                    </h1>
                    <p class="blabla">
                        In case you use all or part of this project, please cite the following paper:
                    </p>
                    <pre>
	                    <code>
@inproceedings{robert2022dva,
    title={Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation},
    author={Robert, Damien and Vallet, Bruno and Landrieu, Loic},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2022},
    pages={5575--5584},
    year={2022},
    url = {\url{https://github.com/drprojects/DeepViewAgg}}
}
	                    </code>
                    </pre>

                    <!-- Acknowledgments -->
                    <h1 id="Acknowledgments">Acknowledgments üôè<a class="anchor-link" href="#Acknowledgments">&#182;</a>
                    </h1>
                    <p class="blabla">
                        This work was funded by {{ page.to_crigen }} and carried on in the {{ page.to_lastig }} research
                        unit of Universit√© Paris-Est, (now {{ page.to_uge }}).</p>
                    <p class="blabla">
                        We thank <a href="https://www.ai4geo.eu">AI4GEO</a> for sharing their computing resources.
                        AI4GEO is a project funded by the French future investment program led by the Secretariat
                        General for Investment and operated by public investment bank Bpifrance.</p>
                    <p class="blabla">
                        We thank <a href="https://www.linkedin.com/in/fihkh">Philippe Calvez</a>,
                        <a href="https://www.linkedin.com/in/dmitriy-slutskiy">Dmitriy Slutskiy</a>,
                        <a href="https://www.linkedin.com/in/marcosgomesborges">Marcos Gomes-Borges</a>,
                        <a href="https://www.linkedin.com/in/gislechuga/">Gisela Lechuga</a>, from
                        {{ page.to_crigen }} and
                        <a href="https://romainloiseau.github.io/">Romain Loiseau</a>,
                        <a href="https://www.linkedin.com/in/vivien-sainte-fare-garnot-41935892">Vivien Sainte Fare
                            Garnot</a>
                        and <a href="https://www.umr-lastig.fr/ewelina-rupnik/">Ewelina Rupnik</a> from the
                        {{ page.to_lastig }} lab at {{ page.to_ign }} for inspiring discussions and valuable feedback.
                    </p>

                </div>
            </div>
        </div>

        <!-- Footer -->
        {% include_relative footer.html %}
    </section>

</div>