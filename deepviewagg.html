---
my_name: Damien Robert
title: Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation
image: "images/deepviewagg/cvpr_2022.png"
icon: "images/deepviewagg/teaser_mini.png"

to_ign: <a href="https://www.ign.fr">IGN</a>
to_ensg: <a href="https://www.ensg.eu/">ENSG</a>
to_lastig: <a href="https://www.umr-lastig.fr">LASTIG</a>
to_strudel: <a href="https://www.umr-lastig.fr/strudel">STRUDEL</a>
to_acte: <a href="https://www.umr-lastig.fr/acte">ACTE</a>
to_uge:  <a href="https://www.univ-gustave-eiffel.fr">Univ. Gustave Eiffel</a>
to_crigen: <a href="https://www.engie.com/en/innovation-transition-energetique/centres-de-recherche/crigen">ENGIE Lab
    CRIGEN</a>
to_loic: <a href="https://loiclandrieu.com/">Lo√Øc Landrieu</a>
to_bruno: <a href="https://www.umr-lastig.fr/bruno-vallet/">Bruno Vallet</a>

to_paper: <a href="https://arxiv.org/abs/2204.07548">Paper</a>
to_webpage: <a href="">Webpage</a>
to_code: <a href="https://github.com/drprojects/DeepViewAgg">Code</a>
to_video: <a href="">Video</a>

to_sota_s3dis: <a
        href="https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?p=learning-multi-view-aggregation-in-the-wild">S3DIS</a>
to_sota_kitti360: <a
        href="https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?p=learning-multi-view-aggregation-in-the-wild">KITTI-360</a>
---
<!-- Imports -->
{% include_relative imports.html %}

<!-- Change colors here -->
<!--<div style="&#45;&#45;color_left:#80cc28;&#45;&#45;color_right:#00aec7">-->
<div style="--color_left:#f2ce02;--color_right:#d46606">

    <!-- Navigation menu -->
    {% include_relative navigation.html %}

    <!-- Header -->
    <section class="page-header">
        <h1 class="project-name">
            {{ page.title }}
        </h1>
        <h2 class="project-tagline">
            <img class=profilepicture src={{ page.image }} style="height:10vh;">
            CVPR 2022 Oral üé§
        </h2>
        <h2 class="project-tagline">
            <a href="">{{ page.my_name }}</a><sup>1,2</sup>, {{ page.to_bruno }}<sup>2</sup>, {{ page.to_loic }}<sup>2</sup>
        </h2>
        <h2 class="project-tagline">
            <sup>1</sup>CSAI, {{ page.to_crigen }}, Stains, France
            <br>
            <sup>2</sup>{{ page.to_uge }}, {{ page.to_ign }}-{{ page.to_ensg }}, {{ page.to_lastig }}, F-94160
            Saint-Mande, France
        </h2>
        <a href=https://github.com/drprojects/DeepViewAgg class="btn">
            <img height="50" src="images/icons/github.svg">Code
        </a>
        <a href=https://arxiv.org/abs/2204.07548 class="btn">
            <img height="50" src="images/icons/paper.svg">Paper
        </a>
        <a href="" class="btn">
            <img height="50" src="images/icons/video.svg">Video
        </a>
    </section>

    <!-- Page content -->
    <section class="main-content">
        <div class="cell border-box-sizing text_cell rendered">
            <div class="inner_cell">
                <div class="text_cell_render border-box-sizing rendered_html">

                    <!-- Interactive visualization -->
                    {% include_relative images/deepviewagg/s3dis_multimodal_sample__pos.html %}

                    <!-- Abstract -->
                    <div class="tom_table">
                        <div class="tom_row">
                            <div class="tom_cell" style="width:40%">
                                <img src="images/deepviewagg/teaser.png" alt="deepviewagg" style="height:30vh">
                            </div>
                            <div class="tom_cell_text">
                                <p>
                                    Recent works on 3D semantic segmentation propose to exploit the synergy between
                                    images üì∏ and point clouds ‚òÅ by processing each modality with a dedicated network
                                    and projecting learned 2D features onto 3D points. Merging large-scale point clouds
                                    and images raises several challenges, such as constructing a mapping between points
                                    and pixels, and aggregating features between multiple views. Current methods require
                                    mesh reconstruction or specialized sensors to recover occlusions, and use heuristics
                                    to select and aggregate available images. In contrast, we propose an end-to-end
                                    trainable multi-view aggregation model leveraging the viewing conditions of 3D
                                    points to merge features from images taken at arbitrary positions. Our method can
                                    combine standard 2D and 3D networks and outperforms both 3D models operating on
                                    colorized point clouds and hybrid 2D/3D networks without requiring colorization,
                                    meshing, or true depth maps. We set a new state-of-the-art for large-scale
                                    indoor/outdoor semantic segmentation on {{ page.to_sota_s3dis }} (74.7 mIoU 6-Fold)
                                    and on {{ page.to_sota_kitti360 }} (58.3 mIoU). Our full pipeline is accessible at
                                    on <a href="https://github.com/drprojects/DeepViewAgg">GitHub</a>, and only requires
                                    raw 3D scans and a set of images and poses.
                                </p>
                            </div>
                        </div>
                    </div>

                    <!-- Resources -->

                    <!-- Acknowledgments -->


                </div>
            </div>
        </div>

        <!-- Footer -->
        {% include_relative footer.html %}
    </section>

</div>